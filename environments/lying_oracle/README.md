# Adaptive Lying Oracle - RL Challenge

Um ambiente de Reinforcement Learning onde um agente deve adaptar sua estratÃ©gia quando um orÃ¡culo muda seu comportamento de verdadeiro para mentiroso.

## ğŸ¯ O Desafio

### DescriÃ§Ã£o

Imagine um jogo onde vocÃª precisa adivinhar um nÃºmero secreto entre 1 e 100, mas sÃ³ pode fazer perguntas do tipo "o nÃºmero Ã© maior que k?". O orÃ¡culo responde, mas tem um twist: **ele comeÃ§a dizendo a verdade, mas depois de 200 perguntas, ele inverte todas as respostas!**

### EspecificaÃ§Ãµes

1. **Hidden Number**: Um nÃºmero aleatÃ³rio entre 1-100
2. **AÃ§Ã£o do Agente**: Escolher `k` (1-100) e perguntar "o nÃºmero Ã© > k?"
3. **Comportamento do OrÃ¡culo**:
   - `t < 200`: Responde **verdade** (truthful)
   - `t â‰¥ 200`: Responde **mentira** (inverte a resposta)
4. **Recompensas**:
   - `+1.0` se `k == hidden_number` (acertou! episÃ³dio termina)
   - `-0.01` caso contrÃ¡rio (penalidade por nÃ£o acertar)
5. **DuraÃ§Ã£o**: MÃ¡ximo de 500 passos por episÃ³dio

### O Objetivo

Criar um agente que:
- âœ… ComeÃ§a com busca binÃ¡ria (assumindo orÃ¡culo verdadeiro)
- âœ… Detecta quando o orÃ¡culo comeÃ§a a mentir
- âœ… Adapta sua estratÃ©gia (inverte interpretaÃ§Ã£o das respostas)
- âœ… Encontra o nÃºmero secreto mesmo com o orÃ¡culo mentiroso

## ğŸ—ï¸ Arquitetura

### 1. Ambiente: `LyingOracleEnv`

```python
class LyingOracleEnv(vf.MultiTurnEnv):
    """
    Ambiente multi-turn que simula o orÃ¡culo adaptativo.
    
    - Oracle responde verdade para t < 200
    - Oracle inverte resposta para t >= 200
    - EpisÃ³dio termina em 500 passos ou quando acerta
    """
```

**CaracterÃ­sticas**:
- Herda de `MultiTurnEnv` do framework Verifiers
- State tracking: turno atual, resposta do orÃ¡culo, histÃ³rico
- Reward: +1 para acerto, -0.01 para erro
- TerminaÃ§Ã£o: acerto ou max_turns

### 2. Agente: `AdaptiveAgent`

```python
class AdaptiveAgent:
    """
    Agente que detecta mudanÃ§a de comportamento e adapta.
    
    EstratÃ©gias de DetecÃ§Ã£o:
    1. ContradiÃ§Ãµes (bounds cruzam)
    2. Queda no reward mÃ©dio
    3. NÃ£o convergÃªncia apÃ³s muitos passos
    """
```

**Mecanismo de AdaptaÃ§Ã£o**:

1. **Fase Inicial (t < 200)**:
   - Busca binÃ¡ria normal
   - Reduz espaÃ§o baseado em respostas verdadeiras
   - Converge rapidamente

2. **DetecÃ§Ã£o (t â‰ˆ 200-220)**:
   - Detecta contradiÃ§Ãµes: `low > high`
   - Rastreia queda no reward mÃ©dio mÃ³vel
   - Contador de contradiÃ§Ãµes atinge threshold

3. **Fase Adaptada (t > 220)**:
   - Switch: `oracle_lying = True`
   - Inverte interpretaÃ§Ã£o: `effective_response = not oracle_response`
   - Busca binÃ¡ria volta a funcionar
   - Encontra o nÃºmero

### 3. Notebook: `demo.ipynb`

DemonstraÃ§Ã£o completa com:
- âœ… ExecuÃ§Ã£o de episÃ³dios
- âœ… Plots de rewards (individual + rolling mean)
- âœ… VisualizaÃ§Ã£o de cumulative reward
- âœ… Estado de adaptaÃ§Ã£o do agente
- âœ… EstatÃ­sticas de performance

## ğŸ“Š VisualizaÃ§Ãµes

O notebook gera 4 plots principais:

1. **Individual Rewards**: Mostra cada reward por turno
2. **Rolling Mean Reward**: 
   - â¬‡ï¸ **Drop em t=200** (orÃ¡culo comeÃ§a a mentir)
   - â¬†ï¸ **Recovery apÃ³s adaptaÃ§Ã£o** (agente detecta e corrige)
3. **Cumulative Reward**: TendÃªncia geral ao longo do tempo
4. **Agent State**: 
   - ContradiÃ§Ãµes acumuladas
   - Momento em que agente acredita que orÃ¡culo estÃ¡ mentindo

### Exemplo de Resultado

```
ğŸ¯ Hidden number: 73
âš™ï¸ Oracle switches to lying mode at t=200

ğŸ”„ AGENT ADAPTATION: Detected lying oracle at turn 215!
âœ… Found hidden number 73 at turn 238!

ğŸ“Š Results:
  - Found: True
  - Turns taken: 238
  - Final cumulative reward: -1.37

ğŸ“ˆ Performance Statistics:

Phase 1 - Truthful Oracle (t < 200):
  Average reward: -0.0100
  Steps: 200

Phase 2 - Oracle Lying, Agent Unaware (200 â‰¤ t < 215):
  Average reward: -0.0100
  Steps: 15
  Contradictions accumulated: 3

Phase 3 - Agent Adapted (t â‰¥ 215):
  Average reward: 0.0435  (recovery!)
  Steps: 24
  Found solution: True
```

## ğŸš€ Como Usar

### 1. InstalaÃ§Ã£o

```bash
cd environments/lying_oracle
uv pip install -e .
```

### 2. Executar Testes

```bash
uv run test_environment.py
```

**SaÃ­da esperada**:
```
Running Lying Oracle Environment Tests...

âœ… Environment created successfully!
âœ… Dataset structure is correct!
âœ… Oracle behavior correct! Hidden number: 45
âœ… Reward calculation correct!
âœ… Episode termination correct!
âœ… Agent found 73 in 7 turns with binary search!
âœ… Agent adapted at turn 215 (oracle started lying at 200)
âœ… Full episode completed! Found 67 in 243 turns

ğŸ‰ All tests passed!
```

### 3. Executar Notebook

```bash
cd environments/lying_oracle
jupyter notebook demo.ipynb
```

Ou use o VS Code / Cursor para executar o notebook interativamente.

### 4. Usar Programaticamente

```python
from lying_oracle import load_environment
from agent import run_episode

# Carregar ambiente
env = load_environment(num_examples=10, max_turns=500, lying_threshold=200)

# Executar um episÃ³dio
hidden_number = 42
history, found, turns = run_episode(
    hidden_number=hidden_number,
    max_turns=500,
    lying_threshold=200,
    verbose=True
)

print(f"Success: {found}, Turns: {turns}")
```

## ğŸ§  Algoritmo do Agente

### PseudocÃ³digo

```python
def adaptive_binary_search(oracle, hidden_number):
    low, high = 1, 100
    oracle_lying = False
    contradictions = 0
    
    for turn in range(500):
        # Escolher k (meio do intervalo atual)
        k = (low + high) // 2
        
        # Obter resposta do orÃ¡culo
        response = oracle.ask(k, turn)
        
        # Se acredita que estÃ¡ mentindo, inverte
        if oracle_lying:
            response = not response
        
        # Atualizar bounds
        if response:  # hidden > k
            low = k + 1
        else:         # hidden <= k
            high = k
        
        # Detectar contradiÃ§Ã£o
        if low > high:
            contradictions += 1
            
        # Switch para modo mentira
        if contradictions >= 3 and not oracle_lying:
            oracle_lying = True
            low, high = 1, 100  # Reset bounds
        
        # Verificar se acertou
        if k == hidden_number:
            return True, turn
    
    return False, 500
```

### EstratÃ©gias de DetecÃ§Ã£o

| EstratÃ©gia | Threshold | DescriÃ§Ã£o |
|------------|-----------|-----------|
| **ContradiÃ§Ãµes** | â‰¥ 3 | Bounds cruzam (low > high) |
| **Reward Drop** | < -0.15 | MÃ©dia mÃ³vel cai muito |
| **NÃ£o ConvergÃªncia** | t > 180 e range > 30 | NÃ£o estÃ¡ convergindo |

## ğŸ“ˆ Resultados Esperados

### MÃ©tricas de Sucesso

Com 5 episÃ³dios:
- **Taxa de Sucesso**: ~100%
- **Turnos MÃ©dios**: ~250-300
- **Delay de AdaptaÃ§Ã£o**: ~10-20 turnos apÃ³s t=200

### Comportamento dos Plots

**Plot 1 - Rolling Mean**:
```
 0.00 |â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      |                    \
-0.01 |                     \_____ (drop at t=200)
      |                          /
      |                         /
      |                        /_____ (recovery after adaptation)
      |â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      0       100      200      300      400      500
```

**Plot 2 - Cumulative Reward**:
```
      |    /
  0.0 |   /
      |  /
      | /  \
 -2.0 |/    \___  (slope changes at t=200)
      |         \_____/\  (recovers after adaptation)
      |                \_____
      |â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      0       100      200      300      400      500
```

## ğŸ”¬ Experimentos Adicionais

### 1. Variar o Threshold de Mentira

```python
# Oracle comeÃ§a a mentir mais cedo
history, found, turns = run_episode(
    hidden_number=42,
    lying_threshold=100,  # Ao invÃ©s de 200
    max_turns=500
)
```

### 2. Ajustar Sensibilidade de DetecÃ§Ã£o

```python
agent = AdaptiveAgent(
    detection_window=10,        # Janela menor (mais sensÃ­vel)
    contradiction_threshold=2,  # Menos contradiÃ§Ãµes necessÃ¡rias
    reward_drop_threshold=-0.10 # Threshold menor
)
```

### 3. Testar com Diferentes Hidden Numbers

```python
# NÃºmeros extremos
for hidden in [1, 50, 100]:
    history, found, turns = run_episode(hidden_number=hidden)
    print(f"Hidden={hidden}: Found in {turns} turns")
```

## ğŸ“ IntegraÃ§Ã£o com Verifiers Framework

Este ambiente Ã© compatÃ­vel com o framework Verifiers para:

### AvaliaÃ§Ã£o com LLMs

```bash
# Usar com modelos via API
uv run vf-eval lying-oracle -m gpt-4o-mini -n 10 -r 1
```

### GeraÃ§Ã£o de Rollouts

```python
from verifiers import load_environment, generate_rollouts

env = load_environment("lying-oracle")
rollouts = generate_rollouts(
    env=env,
    model="claude-sonnet",
    num_examples=100
)
```

### Treinamento RL

```python
from prime_rl import PRLConfig, train

config = PRLConfig(
    model="meta-llama/Llama-3.1-8B-Instruct",
    environment="lying-oracle",
    num_epochs=3,
    batch_size=32
)

trained_model = train(config)
```

## ğŸ§ª Desafios AvanÃ§ados

### NÃ­vel 2: Oracle AleatÃ³rio

Modifique o orÃ¡culo para mentir **aleatoriamente** com probabilidade crescente:

```python
lie_probability = min(0.5, turn / 400)
if random.random() < lie_probability:
    response = not response
```

### NÃ­vel 3: Oracle com PadrÃµes

Oracle mente em padrÃµes especÃ­ficos:
- A cada 3 perguntas
- Apenas para nÃºmeros Ã­mpares
- Com probabilidade baseada no valor de k

### NÃ­vel 4: Multi-Agent

MÃºltiplos agentes competindo para encontrar o nÃºmero primeiro, compartilhando (ou nÃ£o) informaÃ§Ãµes.

## ğŸ“š Conceitos de RL Demonstrados

Este projeto demonstra:

1. **Online Learning**: Agente aprende durante execuÃ§Ã£o
2. **Concept Drift**: Comportamento do ambiente muda ao longo do tempo
3. **Exploration vs Exploitation**: Busca binÃ¡ria (exploitation) vs adaptaÃ§Ã£o (exploration)
4. **Reward Shaping**: -0.01 incentiva eficiÃªncia
5. **State Tracking**: Manter histÃ³rico para detecÃ§Ã£o de padrÃµes
6. **Adaptive Behavior**: Mudar estratÃ©gia baseado em feedback

## ğŸ”— Recursos

- **[Verifiers Framework](https://github.com/PrimeIntellect-ai/verifiers)**
- **[Prime-RL](https://github.com/PrimeIntellect-ai/prime-rl)**
- **[DocumentaÃ§Ã£o Verifiers](https://prime-rl.readthedocs.io/)**

## ğŸ“ LicenÃ§a

MIT License

## ğŸ¤ Contribuindo

Melhorias e extensÃµes sÃ£o bem-vindas! Algumas ideias:

- [ ] Implementar agente com Deep RL (DQN, PPO)
- [ ] Adicionar diferentes estratÃ©gias de detecÃ§Ã£o
- [ ] Criar visualizaÃ§Ãµes interativas
- [ ] Benchmark com diferentes algoritmos
- [ ] Extender para mÃºltiplos orÃ¡culos

---

**VersÃ£o**: 0.1.0  
**Autor**: Challenge implementation for RL + Verifiers  
**Data**: Dezembro 2025


