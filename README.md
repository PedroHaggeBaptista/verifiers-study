# Word Count Environment para Verifiers

Um ambiente de exemplo completo para o framework [Verifiers](https://github.com/PrimeIntellect-ai/verifiers) que demonstra como criar, testar e avaliar ambientes customizados para LLM Reinforcement Learning.

## ğŸ¯ Sobre o Projeto

Este projeto implementa um ambiente completo end-to-end que testa a capacidade de LLMs de contar palavras em textos. Serve como:
- **Template** para criar novos ambientes no Verifiers
- **Exemplo didÃ¡tico** de integraÃ§Ã£o completa com o framework
- **DemonstraÃ§Ã£o** de sistema de recompensas multi-critÃ©rio
- **Guia prÃ¡tico** para uso com diferentes APIs (OpenAI, Anthropic, AWS Bedrock)
- **Pipeline completo**: desde avaliaÃ§Ã£o atÃ© geraÃ§Ã£o de rollouts para treinamento RL

## ğŸ” Fluxo Completo

```
1ï¸âƒ£ CRIAR AMBIENTE          2ï¸âƒ£ AVALIAR MODELO         3ï¸âƒ£ GERAR ROLLOUTS       4ï¸âƒ£ TREINAR COM RL
   (Verifiers)              (vf-eval + LiteLLM)      (generate_rollouts)     (Prime-RL)
       â”‚                            â”‚                        â”‚                      â”‚
       â”œâ”€> word_count.py            â”œâ”€> Claude Sonnet       â”œâ”€> rollouts.jsonl    â”œâ”€> Modelo treinado
       â”œâ”€> Rubric (rewards)         â”œâ”€> Calcula rewards     â””â”€> Formato Prime     â””â”€> Deploy
       â””â”€> Parser XML               â””â”€> EstatÃ­sticas
```

Este projeto demonstra todo o pipeline, desde a criaÃ§Ã£o do ambiente atÃ© a geraÃ§Ã£o de dados prontos para treinamento RL.

## ğŸ“ Estrutura do Projeto

```
word_count_environ/
â”œâ”€â”€ environments/
â”‚   â””â”€â”€ word_count/
â”‚       â”œâ”€â”€ word_count.py          # ğŸ¯ ImplementaÃ§Ã£o principal do ambiente
â”‚       â”œâ”€â”€ test_environment.py    # âœ… Testes unitÃ¡rios
â”‚       â”œâ”€â”€ pyproject.toml         # ğŸ“¦ ConfiguraÃ§Ã£o do pacote
â”‚       â””â”€â”€ README.md              # ğŸ“– DocumentaÃ§Ã£o detalhada
â”œâ”€â”€ rollout_generation/
â”‚   â”œâ”€â”€ generate_rollouts.py       # ğŸ”„ Script para gerar rollouts via LiteLLM
â”‚   â”œâ”€â”€ README.md                  # ğŸ“– DocumentaÃ§Ã£o de geraÃ§Ã£o de rollouts
â”‚   â””â”€â”€ rollouts/
â”‚       â””â”€â”€ rollouts.jsonl         # ğŸ’¾ Rollouts gerados (formato Prime-RL)
â”œâ”€â”€ litellm/
â”‚   â””â”€â”€ litellm_config.yaml        # â˜ï¸  ConfiguraÃ§Ã£o do proxy LiteLLM
â””â”€â”€ README.md                      # ğŸ“‹ Este arquivo
```

## ğŸš€ Quick Start: 4 Passos para RL

### Passo 1: Instalar Ambiente

```bash
cd environments/word_count
uv pip install -e .
```

### Passo 2: Configurar LiteLLM (para AWS Bedrock)

```bash
# Configurar credenciais AWS
export AWS_ACCESS_KEY_ID=your_key
export AWS_SECRET_ACCESS_KEY=your_secret  
export AWS_SESSION_TOKEN=your_token  # se aplicÃ¡vel

# Iniciar proxy (em outro terminal)
litellm --config litellm/litellm_config.yaml --port 8000
```

### Passo 3: Avaliar Modelo

```bash
cd ../..  # voltar para raiz
uv run vf-eval word-count -m claude-sonnet -b http://localhost:8000 -n 5 -r 1
```

### Passo 4: Gerar Rollouts para Treinamento

```bash
cd rollout_generation
uv run generate_rollouts.py
# âœ… Rollouts salvos em: rollouts/rollouts.jsonl
```

**Pronto!** VocÃª tem dados prontos para treinar com [Prime-RL](https://github.com/PrimeIntellect-ai/prime-rl) ğŸ‰

---

## ğŸ§ª Testes e ValidaÃ§Ã£o

### Testes UnitÃ¡rios

Valide que o ambiente estÃ¡ funcionando corretamente:

```bash
cd environments/word_count
uv run test_environment.py
```

**SaÃ­da esperada:**
```
> uv run test_environment.py
Running tests...

Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 496.76 examples/s]
âœ… Environment created successfully!
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1543.44 examples/s]
âœ… Dataset structure is correct!
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 780.92 examples/s]
âœ… Parser works correctly!

ğŸ‰ All tests passed!
```

## ğŸ“‹ CaracterÃ­sticas do Ambiente

### Dataset
- **GeraÃ§Ã£o sintÃ©tica** de textos com contagem controlada de palavras
- **ConfigurÃ¡vel**: nÃºmero de exemplos, min/max palavras por texto
- **Formato Verifiers**: campos `question`, `answer`, `info`, `task`

### Parser
- **XMLParser** com tags `<word_count>N</word_count>`
- Extrai respostas numÃ©ricas de forma estruturada
- Tolerante a variaÃ§Ãµes de formataÃ§Ã£o

### Sistema de Recompensas (Rubric)

| FunÃ§Ã£o | Peso | DescriÃ§Ã£o |
|--------|------|-----------|
| **Exact Match** | 1.0 | Resposta exata (1.0 ou 0.0) |
| **Format** | 0.2 | Uso correto das tags XML |
| **Partial Credit** | 0.1 | CrÃ©dito parcial: 1.0 (exato), 0.5 (Â±1), 0.2 (Â±2) |

**Recompensa total mÃ¡xima:** 1.3 (1.0 + 0.2 + 0.1)

## ğŸŒ AvaliaÃ§Ã£o com LLMs via API

O framework Verifiers usa APIs compatÃ­veis com OpenAI para fazer chamadas aos modelos. Para APIs que nÃ£o seguem esse padrÃ£o (como AWS Bedrock), Ã© necessÃ¡rio usar um **proxy de traduÃ§Ã£o**.

### Por que LiteLLM?

**LiteLLM** Ã© um proxy universal que converte requisiÃ§Ãµes OpenAI para diversos provedores:
- âœ… AWS Bedrock (Amazon)
- âœ… Vertex AI (Google)
- âœ… Azure OpenAI
- âœ… Anthropic, Cohere, e 100+ outros

Isso permite usar o `vf-eval` com qualquer provedor sem modificar cÃ³digo.

### ConfiguraÃ§Ã£o do LiteLLM

#### 1. Instalar LiteLLM

```bash
pip install 'litellm[proxy]'
```

#### 2. Criar arquivo de configuraÃ§Ã£o

Crie `litellm/litellm_config.yaml`:

```yaml
model_list:
  - model_name: claude-sonnet
    litellm_params:
      model: bedrock/us.anthropic.claude-sonnet-4-5-20250929-v1:0
      aws_region_name: us-east-1
```

#### 3. Configurar credenciais AWS

```bash
export AWS_ACCESS_KEY_ID=your_access_key_id
export AWS_SECRET_ACCESS_KEY=your_secret_access_key
export AWS_SESSION_TOKEN=your_session_token  # se aplicÃ¡vel
```

#### 4. Iniciar o proxy

```bash
litellm --config litellm/litellm_config.yaml --port 8000
```

O proxy estarÃ¡ rodando em `http://localhost:8000` e traduzirÃ¡ chamadas OpenAI para AWS Bedrock.

### Executando a AvaliaÃ§Ã£o

Com o proxy LiteLLM rodando, execute:

```bash
uv run vf-eval word-count \
  -m claude-sonnet \
  -b http://localhost:8000 \
  -k LITELLM_API_KEY \
  -n 5 -r 1
```

**ParÃ¢metros:**
- `-m`: Nome do modelo (definido no `litellm_config.yaml`)
- `-b`: URL base do proxy LiteLLM
- `-k`: VariÃ¡vel de ambiente com a chave API (pode ser dummy para Bedrock)
- `-n`: NÃºmero de exemplos a avaliar
- `-r`: Rollouts por exemplo (quantas vezes avaliar cada exemplo)

### Exemplo de Resultado Real

```
2025-12-16 00:35:19 - verifiers.utils.env_utils - INFO - Loading environment: word-count
2025-12-16 00:35:19 - verifiers.utils.env_utils - INFO - Using default args: min_words=5, seed=42, max_words=50, num_examples=100
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 7713.52 examples/s]
2025-12-16 00:35:19 - verifiers.utils.env_utils - INFO - Successfully loaded environment 'word-count'
2025-12-16 00:35:19 - verifiers.utils.eval_utils - INFO - Starting evaluation with model: claude-sonnet
2025-12-16 00:35:19 - verifiers.utils.eval_utils - INFO - Configuration: num_examples=5, rollouts_per_example=1, max_concurrent=32
Processing 5 groups (5 total rollouts): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.12it/s]
2025-12-16 00:35:21 - verifiers.utils.eval_utils - INFO - Evaluation completed in 2.36 seconds

--- Evaluation ---
Environment: word-count
Model: claude-sonnet
Provider: http://localhost:8000
Examples: 5
Rollouts per example: 1

--- Example ---
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“         â”‚
â”‚ â”ƒ Prompt                            â”ƒ Completion                      â”ƒ Reward â”ƒ         â”‚
â”‚ â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©         â”‚
â”‚ â”‚ system: You are a word counting   â”‚ assistant: I'll count the words â”‚   1.30 â”‚         â”‚
â”‚ â”‚ assistant. Count the number of    â”‚ in the given text.              â”‚        â”‚         â”‚
â”‚ â”‚ words in the given text and       â”‚                                 â”‚        â”‚         â”‚
â”‚ â”‚ provide your answer in the        â”‚ The text contains the word      â”‚        â”‚         â”‚
â”‚ â”‚ following format:                 â”‚ "word" repeated multiple times. â”‚        â”‚         â”‚
â”‚ â”‚                                   â”‚ Let me count each occurrence:   â”‚        â”‚         â”‚
â”‚ â”‚ <word_count>                      â”‚                                 â”‚        â”‚         â”‚
â”‚ â”‚ [number]                          â”‚ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,  â”‚        â”‚         â”‚
â”‚ â”‚ </word_count>                     â”‚ 11, 12, 13, 14, 15, 16, 17, 18, â”‚        â”‚         â”‚
â”‚ â”‚                                   â”‚ 19, 20, 21, 22, 23, 24, 25, 26, â”‚        â”‚         â”‚
â”‚ â”‚ user: Count the number of words   â”‚ 27, 28, 29, 30, 31, 32, 33, 34, â”‚        â”‚         â”‚
â”‚ â”‚ in the following text:            â”‚ 35, 36, 37, 38, 39, 40, 41, 42, â”‚        â”‚         â”‚
â”‚ â”‚                                   â”‚ 43, 44, 45                      â”‚        â”‚         â”‚
â”‚ â”‚ word word word word word word ... â”‚                                 â”‚        â”‚         â”‚
â”‚ â”‚                                   â”‚ <word_count>                    â”‚        â”‚         â”‚
â”‚ â”‚                                   â”‚ 45                              â”‚        â”‚         â”‚
â”‚ â”‚                                   â”‚ </word_count>                   â”‚        â”‚         â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

--- All ---
Rewards:
reward: avg - 1.300, std - 0.000
r1: [1.3, 1.3, 1.3, 1.3, 1.3]

exact_match_reward: avg - 1.000, std - 0.000
r1: [1.0, 1.0, 1.0, 1.0, 1.0]

format_reward: avg - 1.000, std - 0.000
r1: [1.0, 1.0, 1.0, 1.0, 1.0]

partial_credit_reward: avg - 1.000, std - 0.000
r1: [1.0, 1.0, 1.0, 1.0, 1.0]
```

**AnÃ¡lise dos Resultados:**
- âœ… **Reward mÃ©dio: 1.30** (mÃ¡ximo possÃ­vel!)
- âœ… **100% de acerto** em todas as mÃ©tricas
- âœ… Claude Sonnet contou corretamente todas as palavras
- âœ… FormataÃ§Ã£o XML perfeita em todos os casos
- âš¡ AvaliaÃ§Ã£o completada em **2.36 segundos**

## ğŸ”„ GeraÃ§Ã£o de Rollouts para Treinamento RL

Depois de avaliar seu modelo, vocÃª pode gerar **rollouts** para treinar com o [Prime-RL](https://github.com/PrimeIntellect-ai/prime-rl).

### O que sÃ£o Rollouts?

**Rollouts** sÃ£o interaÃ§Ãµes completas registradas que servem como dados de treinamento para RL:
- ğŸ“ **Prompt**: A entrada do usuÃ¡rio
- ğŸ’¬ **Completion**: A resposta do modelo
- â­ **Reward**: PontuaÃ§Ã£o calculada pelo rubric
- ğŸ“Š **Metadados**: InformaÃ§Ãµes adicionais

### GeraÃ§Ã£o AutomÃ¡tica de Rollouts

O script `generate_rollouts.py` usa o Verifiers para gerar rollouts automaticamente:

```bash
# 1. Certifique-se que o LiteLLM estÃ¡ rodando
litellm --config litellm/litellm_config.yaml --port 8000

# 2. Gere rollouts
cd rollout_generation
uv run generate_rollouts.py
```

**SaÃ­da:**
```
ğŸš€ Gerando 10 rollouts via http://localhost:8000
âš™ï¸  ConcorrÃªncia: 2 requisiÃ§Ãµes simultÃ¢neas
â³ Chamando modelo claude-sonnet...
Processing 10 groups (10 total rollouts): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:13<00:00, 1.31s/it]

âœ… Rollouts salvos em: rollouts/rollouts.jsonl
   Formato: JSONL (uma linha = um rollout)
   Total: 10 rollouts

ğŸ“Š EstatÃ­sticas:
   Reward mÃ©dio: 1.300
   Reward std: 0.000
   Min: 1.300
   Max: 1.300
```

### Formato JSONL

Cada linha do arquivo `rollouts.jsonl` contÃ©m um rollout completo:

```json
{
  "prompt": [
    {"role": "system", "content": "You are a word counting assistant..."},
    {"role": "user", "content": "Count the words: word word word..."}
  ],
  "completion": [
    {"role": "assistant", "content": "<word_count>\n45\n</word_count>"}
  ],
  "reward": 1.3,
  "answer": "45"
}
```

### ConfiguraÃ§Ã£o

Edite as variÃ¡veis no final de `generate_rollouts.py`:

```python
NUM_EXAMPLES = 10          # Quantos rollouts gerar
MODEL = "claude-sonnet"    # Nome do modelo no litellm_config.yaml
BASE_URL = "http://localhost:8000"
MAX_CONCURRENT = 2         # ConcorrÃªncia (ajuste se rate limit)
```

### Uso com Prime-RL

Os rollouts gerados estÃ£o prontos para treinar com Prime-RL:

```toml
# config.toml para Prime-RL
[model]
name = "meta-llama/Llama-3.1-8B-Instruct"

[environment]
env_id = "word-count"

[data]
rollout_dir = "./rollout_generation/rollouts"
rollout_files = ["rollouts.jsonl"]
batch_size = 32

[training]
algorithm = "ppo"
num_epochs = 3
learning_rate = 1e-5

[logging]
wandb_project = "word-count-rl"
```

Execute o treinamento:

```bash
# Instalar Prime-RL
pip install prime-rl

# Treinar modelo
uv run trainer @ config.toml
```

### Dicas para GeraÃ§Ã£o de Rollouts

**Se encontrar Rate Limit (AWS Bedrock):**
- Reduza `NUM_EXAMPLES` (ex: 5)
- Reduza `MAX_CONCURRENT` (ex: 1)
- Gere em batches menores

**Para dados de qualidade variada:**
- Ajuste `temperature` do modelo para ter respostas mais diversas
- Use diferentes `seed` para variar os exemplos
- Misture rollouts de diferentes modelos

**Para produÃ§Ã£o contÃ­nua:**
- Use o Orchestrator do Prime-RL para coleta automÃ¡tica
- Implemente o middleware mostrado na seÃ§Ã£o "ImplementaÃ§Ã£o em ProduÃ§Ã£o"
- Configure coleta periÃ³dica via cron ou serviÃ§o

## âš™ï¸ Outras OpÃ§Ãµes de API

### OpenAI (Direto)
```bash
export OPENAI_API_KEY=your_key
uv run vf-eval word-count -m gpt-4o-mini -n 10 -r 2
```

### Anthropic (Direto)
```bash
export ANTHROPIC_API_KEY=your_key
uv run vf-eval word-count -m claude-sonnet-4-20250514 -n 10 -r 2
```

### vLLM (Modelo Local)
```bash
# 1. Inicie o servidor vLLM
vllm serve meta-llama/Llama-3.1-8B-Instruct --port 8000

# 2. Execute o eval
uv run vf-eval word-count \
  -m meta-llama/Llama-3.1-8B-Instruct \
  -b http://localhost:8000/v1 \
  -n 10 -r 2
```

## ğŸ”§ ConfiguraÃ§Ã£o do Ambiente

### Argumentos CustomizÃ¡veis

Configure via linha de comando com `-a`:

```bash
uv run vf-eval word-count \
  -m claude-sonnet \
  -b http://localhost:8000 \
  -k LITELLM_API_KEY \
  -n 10 -r 2 \
  -a '{"num_examples": 50, "min_words": 3, "max_words": 20, "seed": 123}'
```

| ParÃ¢metro | Tipo | PadrÃ£o | DescriÃ§Ã£o |
|-----------|------|--------|-----------|
| `num_examples` | int | 100 | NÃºmero de exemplos a gerar |
| `min_words` | int | 5 | MÃ­nimo de palavras por exemplo |
| `max_words` | int | 50 | MÃ¡ximo de palavras por exemplo |
| `seed` | int | 42 | Seed para reprodutibilidade |

## ğŸ“Š MÃ©tricas Reportadas

O ambiente calcula e reporta as seguintes mÃ©tricas em cada avaliaÃ§Ã£o:

| MÃ©trica | DescriÃ§Ã£o | Faixa |
|---------|-----------|-------|
| `reward` | Recompensa total ponderada (soma de todas as recompensas Ã— pesos) | 0.0 - 1.3 |
| `exact_match_reward` | Resposta exata: 1.0 se correto, 0.0 se incorreto | 0.0 - 1.0 |
| `format_reward` | Uso correto das tags XML `<word_count>N</word_count>` | 0.0 - 1.0 |
| `partial_credit_reward` | CrÃ©dito parcial baseado na proximidade da resposta | 0.0 - 1.0 |

Resultados sÃ£o automaticamente salvos em formato compatÃ­vel com HuggingFace Datasets.

## ğŸ“ Usando como Template

Este ambiente pode servir como base para criar seus prÃ³prios ambientes:

### Passo a Passo

1. **Copie a estrutura** de `environments/word_count/`
2. **Modifique `load_environment()`** com sua lÃ³gica de dataset
3. **Ajuste o parser** conforme o formato de saÃ­da desejado
4. **Defina funÃ§Ãµes de recompensa** especÃ­ficas para sua tarefa
5. **Atualize o `pyproject.toml`** com o novo nome e metadados
6. **Instale** com `uv pip install -e .`
7. **Teste** com `vf-eval seu-ambiente`

### Componentes Principais

```python
# environments/seu_ambiente/seu_ambiente.py
import verifiers as vf
from datasets import Dataset

def load_environment(**kwargs) -> vf.Environment:
    # 1. Crie ou carregue seu dataset
    dataset = Dataset.from_list([...])
    
    # 2. Defina o parser (XMLParser, ThinkParser, ou custom)
    parser = vf.XMLParser(["answer_field"])
    
    # 3. Defina funÃ§Ãµes de recompensa
    def reward_func(completion, answer, **kwargs) -> float:
        # Sua lÃ³gica aqui
        return score
    
    # 4. Crie o rubric
    rubric = vf.Rubric(
        funcs=[reward_func],
        weights=[1.0],
        parser=parser
    )
    
    # 5. Retorne o ambiente
    return vf.SingleTurnEnv(
        dataset=dataset,
        rubric=rubric,
        parser=parser,
        system_prompt="...",
        **kwargs
    )
```

## ğŸ”— Recursos Ãšteis

- **[Verifiers Framework](https://github.com/PrimeIntellect-ai/verifiers)** - Framework para ambientes e avaliaÃ§Ã£o
- **[Prime-RL](https://github.com/PrimeIntellect-ai/prime-rl)** - Framework para treinamento RL em escala
- **[DocumentaÃ§Ã£o Verifiers](https://prime-rl.readthedocs.io/)** - Guias e referÃªncias
- **[Exemplos de Ambientes](https://github.com/PrimeIntellect-ai/verifiers/tree/main/environments)** - Mais templates
- **[LiteLLM](https://docs.litellm.ai/)** - DocumentaÃ§Ã£o do proxy universal
- **[rollout_generation/README.md](./rollout_generation/README.md)** - DocumentaÃ§Ã£o detalhada de geraÃ§Ã£o de rollouts

## ğŸš€ ImplementaÃ§Ã£o em ProduÃ§Ã£o

### Arquitetura para Coleta de Dados de RL

Para usar este ambiente em produÃ§Ã£o e coletar dados para treinamento com Reinforcement Learning, vocÃª pode implementar um sistema de **middleware de avaliaÃ§Ã£o** que intercepta todas as inferÃªncias do seu agente.

#### Arquitetura Proposta

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cliente   â”‚ â”€â”€â”€> â”‚  API Gateway     â”‚ â”€â”€â”€> â”‚   Agente    â”‚
â”‚  (UsuÃ¡rio)  â”‚      â”‚  + Middleware    â”‚      â”‚    (LLM)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”œâ”€> Calcula Reward (Rubric)
                              â”‚
                              â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Banco de Dados â”‚
                     â”‚  (Rewards Store) â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ Batch Processing
                              â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  Prime Framework â”‚
                     â”‚  (RL Training)   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplementaÃ§Ã£o Passo a Passo

#### 1. Middleware de AvaliaÃ§Ã£o

Crie um middleware que intercepta as requisiÃ§Ãµes/respostas do agente:

```python
# production/middleware.py
import asyncio
from datetime import datetime
from typing import Dict, Any
import verifiers as vf
from word_count import load_environment

class RewardMiddleware:
    def __init__(self):
        # Carregue o ambiente sem dataset (apenas rubric)
        self.env = load_environment(num_examples=0)
        self.rubric = self.env.rubric
        self.parser = self.env.parser
        
    async def evaluate_interaction(
        self,
        prompt: str,
        completion: str,
        metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Avalia uma interaÃ§Ã£o em tempo real.
        
        Args:
            prompt: A pergunta/prompt do usuÃ¡rio
            completion: A resposta do agente
            metadata: Metadados adicionais (user_id, session_id, etc)
        
        Returns:
            Dict com rewards e metadados
        """
        # Formato das mensagens para o rubric
        completion_msgs = [{"role": "assistant", "content": completion}]
        
        # Extrair resposta esperada (se disponÃ­vel) ou usar heurÃ­sticas
        answer = self._extract_ground_truth(prompt, metadata)
        
        # Calcular rewards usando o rubric
        rewards = {}
        for func, weight in zip(self.rubric.funcs, self.rubric.weights):
            try:
                reward_value = func(
                    completion=completion_msgs,
                    answer=answer,
                    prompt=prompt,
                    info=metadata
                )
                rewards[func.__name__] = {
                    "value": reward_value,
                    "weight": weight,
                    "weighted_value": reward_value * weight
                }
            except Exception as e:
                rewards[func.__name__] = {
                    "error": str(e),
                    "value": 0.0,
                    "weight": weight
                }
        
        # Calcular reward total
        total_reward = sum(r["weighted_value"] for r in rewards.values())
        
        return {
            "prompt": prompt,
            "completion": completion,
            "rewards": rewards,
            "total_reward": total_reward,
            "metadata": metadata,
            "timestamp": datetime.utcnow().isoformat()
        }
    
    def _extract_ground_truth(self, prompt: str, metadata: Dict) -> str:
        """
        Extrai ou infere a resposta correta.
        Em produÃ§Ã£o, pode vir de metadados, validaÃ§Ã£o humana, ou heurÃ­sticas.
        """
        return metadata.get("expected_answer", "")
```

#### 2. IntegraÃ§Ã£o com API

Integre o middleware na sua API principal:

```python
# production/api.py
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
import asyncio

from middleware import RewardMiddleware
from storage import RewardStorage

app = FastAPI()
reward_middleware = RewardMiddleware()
storage = RewardStorage()

class InferenceRequest(BaseModel):
    prompt: str
    user_id: str
    session_id: str
    expected_answer: str = None  # Opcional: para validaÃ§Ã£o

class InferenceResponse(BaseModel):
    completion: str
    reward_score: float = None
    interaction_id: str

@app.post("/inference", response_model=InferenceResponse)
async def inference(
    request: InferenceRequest,
    background_tasks: BackgroundTasks
):
    # 1. Fazer inferÃªncia no agente/LLM
    completion = await call_agent(request.prompt)
    
    # 2. Calcular reward em background (nÃ£o bloqueia resposta)
    interaction_id = generate_id()
    
    background_tasks.add_task(
        evaluate_and_store,
        interaction_id=interaction_id,
        prompt=request.prompt,
        completion=completion,
        metadata={
            "user_id": request.user_id,
            "session_id": request.session_id,
            "expected_answer": request.expected_answer
        }
    )
    
    return InferenceResponse(
        completion=completion,
        interaction_id=interaction_id
    )

async def evaluate_and_store(
    interaction_id: str,
    prompt: str,
    completion: str,
    metadata: dict
):
    """Avalia e armazena rewards em background."""
    # Avaliar
    result = await reward_middleware.evaluate_interaction(
        prompt=prompt,
        completion=completion,
        metadata=metadata
    )
    
    # Armazenar
    await storage.store_interaction(
        interaction_id=interaction_id,
        data=result
    )
```

#### 3. Armazenamento de Dados

Configure um banco de dados para armazenar as interaÃ§Ãµes e rewards:

```python
# production/storage.py
import asyncpg
from typing import Dict, Any
import json

class RewardStorage:
    def __init__(self, db_url: str):
        self.db_url = db_url
        self.pool = None
    
    async def connect(self):
        self.pool = await asyncpg.create_pool(self.db_url)
    
    async def store_interaction(
        self,
        interaction_id: str,
        data: Dict[str, Any]
    ):
        """Armazena interaÃ§Ã£o e rewards no banco."""
        async with self.pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO rl_interactions (
                    interaction_id,
                    prompt,
                    completion,
                    total_reward,
                    rewards_detail,
                    metadata,
                    timestamp
                ) VALUES ($1, $2, $3, $4, $5, $6, $7)
            """,
                interaction_id,
                data["prompt"],
                data["completion"],
                data["total_reward"],
                json.dumps(data["rewards"]),
                json.dumps(data["metadata"]),
                data["timestamp"]
            )
    
    async def get_training_batch(
        self,
        batch_size: int = 1000,
        min_reward: float = None
    ):
        """Recupera batch de dados para treinamento."""
        async with self.pool.acquire() as conn:
            query = """
                SELECT 
                    prompt,
                    completion,
                    total_reward,
                    rewards_detail,
                    metadata
                FROM rl_interactions
                WHERE processed = FALSE
            """
            if min_reward is not None:
                query += f" AND total_reward >= {min_reward}"
            
            query += f" LIMIT {batch_size}"
            
            rows = await conn.fetch(query)
            return [dict(row) for row in rows]
```

#### 4. Schema do Banco de Dados

```sql
-- production/schema.sql
CREATE TABLE rl_interactions (
    interaction_id VARCHAR(255) PRIMARY KEY,
    prompt TEXT NOT NULL,
    completion TEXT NOT NULL,
    total_reward FLOAT NOT NULL,
    rewards_detail JSONB NOT NULL,
    metadata JSONB,
    timestamp TIMESTAMP NOT NULL,
    processed BOOLEAN DEFAULT FALSE,
    used_in_training BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Ãndices para queries rÃ¡pidas
CREATE INDEX idx_reward ON rl_interactions(total_reward);
CREATE INDEX idx_timestamp ON rl_interactions(timestamp);
CREATE INDEX idx_processed ON rl_interactions(processed);
CREATE INDEX idx_user_id ON rl_interactions((metadata->>'user_id'));
```

#### 5. IntegraÃ§Ã£o com Prime Framework

Script para exportar dados e treinar com o Prime Framework:

```python
# production/train_with_prime.py
import asyncio
from datasets import Dataset
import verifiers as vf
from storage import RewardStorage

async def export_to_prime_format(storage: RewardStorage, output_path: str):
    """Exporta dados coletados para formato Prime."""
    
    # 1. Recuperar dados do banco
    interactions = await storage.get_training_batch(
        batch_size=10000,
        min_reward=0.5  # Apenas interaÃ§Ãµes com reward razoÃ¡vel
    )
    
    # 2. Converter para formato Verifiers
    dataset_dict = []
    for interaction in interactions:
        dataset_dict.append({
            "prompt": [{"role": "user", "content": interaction["prompt"]}],
            "completion": [{"role": "assistant", "content": interaction["completion"]}],
            "reward": interaction["total_reward"],
            "info": interaction["metadata"]
        })
    
    # 3. Criar Dataset HuggingFace
    dataset = Dataset.from_list(dataset_dict)
    
    # 4. Salvar
    dataset.save_to_disk(output_path)
    print(f"âœ… Dataset exportado: {len(dataset)} exemplos em {output_path}")
    
    return dataset

async def train_with_prime():
    """Executa treinamento RL com Prime Framework."""
    storage = RewardStorage(db_url="postgresql://...")
    await storage.connect()
    
    # Exportar dados
    dataset = await export_to_prime_format(
        storage=storage,
        output_path="./training_data/word_count_prod"
    )
    
    # Carregar ambiente
    env = vf.load_environment("word-count")
    
    # Configurar e executar treinamento com Prime
    # (ver documentaÃ§Ã£o do Prime Framework para detalhes)
    """
    from prime_rl import PRLConfig, train
    
    config = PRLConfig(
        model="meta-llama/Llama-3.1-8B-Instruct",
        dataset=dataset,
        environment=env,
        num_epochs=3,
        batch_size=32,
        learning_rate=1e-5
    )
    
    trained_model = train(config)
    """

if __name__ == "__main__":
    asyncio.run(train_with_prime())
```

### Fluxo de ProduÃ§Ã£o Completo

1. **Coleta em Tempo Real**
   - UsuÃ¡rio faz pergunta â†’ API processa
   - Agente gera resposta â†’ Enviada ao usuÃ¡rio
   - Middleware calcula reward em background
   - Dados armazenados no banco

2. **AnÃ¡lise e Filtro**
   - Dashboard para monitorar distribuiÃ§Ã£o de rewards
   - Identificar casos de baixo reward para revisÃ£o
   - Filtrar dados de qualidade para treinamento

3. **PreparaÃ§Ã£o para Treinamento**
   - Exportar batch de interaÃ§Ãµes (ex: Ãºltimos 7 dias)
   - Balancear dataset (diferentes faixas de reward)
   - Converter para formato HuggingFace Dataset

4. **Treinamento com Prime**
   - Usar dados coletados como experiÃªncias
   - Treinar modelo com PPO/GRPO
   - Avaliar modelo melhorado
   - Deploy gradual (A/B testing)

### ConsideraÃ§Ãµes de ProduÃ§Ã£o

#### Performance
- **Rewards em background**: NÃ£o bloqueie a resposta ao usuÃ¡rio
- **Cache**: Use Redis para rubrics frequentes
- **Batch processing**: Processe mÃºltiplas avaliaÃ§Ãµes em paralelo

#### Privacidade e SeguranÃ§a
- **AnonimizaÃ§Ã£o**: Remova PII antes de armazenar
- **RetenÃ§Ã£o**: Defina polÃ­ticas de retenÃ§Ã£o de dados
- **Auditoria**: Logs de todas as avaliaÃ§Ãµes

#### Monitoramento
```python
# MÃ©tricas importantes
- DistribuiÃ§Ã£o de rewards (histograma)
- Taxa de interaÃ§Ãµes com reward > threshold
- LatÃªncia do middleware
- Volume de dados coletados por dia
```

#### Qualidade dos Dados
- **ValidaÃ§Ã£o humana**: Amostras aleatÃ³rias revisadas por humanos
- **Ground truth**: Para tarefas com resposta certa, valide contra ela
- **Feedback do usuÃ¡rio**: Capture thumbs up/down para ajustar rewards

### Exemplo de ConfiguraÃ§Ã£o Docker

```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build: ./production
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/rl_data
    ports:
      - "8000:8000"
    depends_on:
      - db
  
  db:
    image: postgres:15
    environment:
      - POSTGRES_DB=rl_data
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
  
  training_worker:
    build: ./production
    command: python train_with_prime.py
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/rl_data
    depends_on:
      - db

volumes:
  postgres_data:
```

Este setup permite coletar dados de produÃ§Ã£o continuamente e retreinar seu modelo periodicamente, criando um **ciclo de melhoria contÃ­nua** com Reinforcement Learning.

## ğŸ’¡ Dicas e SoluÃ§Ã£o de Problemas

### Erros Comuns

**Erro: "No module named 'word_count'"**
```bash
cd environments/word_count && uv pip install -e .
```

**Erro: "Connection refused" no LiteLLM**
```bash
# Verifique se o proxy estÃ¡ rodando
litellm --config litellm/litellm_config.yaml --port 8000
```

**Erro: AWS Credentials**
```bash
# Configure as credenciais AWS
export AWS_ACCESS_KEY_ID=...
export AWS_SECRET_ACCESS_KEY=...
```

### Performance

- Use `-n` menor para testes rÃ¡pidos (ex: `-n 5`)
- Aumente `max_concurrent` para paralelismo maior
- Use `temperature=0` para resultados determinÃ­sticos

## ğŸ“ LicenÃ§a

MIT License

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Este projeto Ã© um exemplo educacional.

---

## ğŸ“Œ Status do Projeto

| Componente | Status | Detalhes |
|------------|--------|----------|
| **Ambiente** | âœ… Pronto | word_count environment implementado e testado |
| **AvaliaÃ§Ã£o** | âœ… Pronto | vf-eval funcionando com LiteLLM + AWS Bedrock |
| **GeraÃ§Ã£o de Rollouts** | âœ… Pronto | Script automÃ¡tico gerando JSONL para Prime-RL |
| **Treinamento RL** | ğŸ“‹ Documentado | Guia completo para uso com Prime-RL |
| **ProduÃ§Ã£o** | ğŸ“‹ Arquitetura | Middleware e pipeline documentados |

**VersÃ£o:** 0.1.0  
**Frameworks:** Verifiers >= 0.1.8.post2 | Prime-RL (compatÃ­vel)  
**Ãšltima atualizaÃ§Ã£o:** Dezembro 2025